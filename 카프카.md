## 아파치 카프카 TOOL
<em><u>모두 JAVA기반이다.</u></em> <br>
`프로듀서` : 데이터 넣는역할 <br>
`토픽` : 프로듀서가 넣은 데이터가 토픽에 들어간다<br>
`컨슈머` : 토픽에 있는 데이터를 가져가는 역할<br>
`스트림즈` : 토픽에 있는 데이터를 StateFull, StateLess 하게 어떠한 처리를 해서 다시 토픽에 넣는역할 <br>
`커넥트` : 
1. 데이터 파이프라인을 연결하는 가장 핵심적인 tool 이다
2. 일반적인 프로듀서, 컨슈머와의 차이점은 클러스터로 운영이 가능하고, 템플릿형태로써 반복적으로 여러번 생성이 가능하다
```
프로듀서, 컨슈머를 개별로 운영하는것보다 훨씬 효율적인데, REST-API 
로 커넥트에 통신을 하여 반복적으로 만들 수 있다는 장점이 있다
```
* 커넥트(소스) : 
  * 프로듀서 역할 
  * 특정 데이터베이스나 소스 애플리케이션으로 부터  데이터를 가져와서 토픽에 넣는역할
* 커넥트(싱크) : 
  * 컨슈머 역할
  * 타겟 애플리케이션으로 데이터를 보내는 역할
___

## 카프카 브로커와 클러스터
`주키퍼` : 카프카 클러스터를 운영하기 위해 반드시 필요한 애플리케이션, 버전2까지는 필수이며 버전3부터는 주키퍼 없이 클러스터 운영이 가능하다(단, 아직 완벽하진 않음)
```
- 한개의 카프카 클러스터는 여러개의 브로커로 이루어져 있다
- 한개의 브로커는 한개의 Physical-Machine 이나 서버 혹은 Instance에서 동작하게 된다
- 상용환경에서 최소한 3개의 브로커를 운용하는게 일반적이고 데이터량이 많고 확장이 
 필요한 경우에는 50~100개까지 운영하는 경우도 있다
- 또는 상황에 따라서 클러스터를 2~3개로 나눠서 운영하는 경우도있다
```
`브로커` : 데이터를 분산저장하여 장애가 나더라도 데이터를 안전하게 사용할 수 있도록 도와주는 애플리케이션
___
## 카프카 클러스터와 주키퍼
`클러스터` : 
* 카프카 클러스터를 실행하기 위해서는 주키퍼가 필요함
* 여러대를 동시에 운영하는 경우도 많음(주키퍼 앙상블) 여러대를 운영하기 위해서는
* 주키퍼의 서로 다른 znode에 클러스터를 지정하면 되며, root znode에 <br>
각 클러스터별 znode를 생성하고 클러스터 실행시 root가 아닌 하위 znode로 설정
* 카프카3버전 부터는 주키퍼가 없어도 클러스터 동작 가능
___
## 브로커의 역할 - 컨트롤러, 데이터 삭제
`컨트롤러` :
* 다수의 브로커중 한대가 컨트롤러 역할을 한다
* 다른 브로커들의 상태체크, 브로커가 클러스터에서 빠지는 경우에(어떠한 이슈가 생겼을 경우) 브로커에 존재하는<br> 리더파티션을 재분배 한다
* 카프카는 지속적으로 데이터를 처리해야 하므로 비정상적인 브로커를 빠르게 클러스터에서 빼내는 것이 중요하다
* 컨트롤러 역할을 하는 브로커가 장애가 생기면 다른 브로커가 역할을 대신한다
```
데이터삭제:
- 다른 메시징 플랫폼과는 다르게 컨슈머가 데이터를 가져가도 토픽의 데이터는 삭제되지 않는다
- 데이터 삭제는 오직 브로커만 가능하며 파일 단위로 이루어지는데 이 단위를  '로그 세그먼트(log segment)'라고 한다
- 세그먼트에는 다수의 데이터가 들어있기 때문에 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없다
- CleanUp Policy라고 하는 delete 옵션에 의해서 특정 시간, 용량에 따라서 삭제를 수행할 수 있고 혹은 특수한 상황에서 Compact 옵션을 주게되면 가장 최신의 메시지KEY 가 있는 레코드를 제외하고 나머지 메시지KEY 가 있는 레코드를 삭제 할 수도있다
```
`컨슈머의 오프셋 저장` :
* 컨슈머가 토픽에서 데이터를 가져가면 어느 위치까지 가져갔는지 commit을 하게되는데 이때 __consumer_offsets 토픽에 저장한다.
* __consumer_offsets 토픽은 자동으로 생성되어 자동으로 관리되기 때문에 Internal 토픽이라고 부른다

`그룹 코디네이커`:
* 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할을 한다
* 일반적으로 파티션과 컨슈머는 1:1 매칭되는데 특정 컨슈머에 문제가 생길경우 문제가 발생한 컨슈머와 매칭되어있던 파티션이 다른 문제없는 파티션을 바라보도록 재할당(`리밸런스(rebalance)`) 하여 데이터가 끊임없이 처리되도록 도와준다
___

## 브로커의 역할 - 데이터의 저장
* 카프카를 실행할 때 config/server.properties의 log.dir 옵션에 정의한 디렉토리에 데이터를 저장한다
* 토픽의 이름과 파티션 번호의 조합으로 디렉토리가 생성된다
  * 예시) 파티션이 3개라면 hello.kafka-0, hello.kafka-1, hello.kafka-2
* 그 하위에 .index, .log, .timeindex, leader-epoch-checkpoint 와 같이 로그와 관련된 데이터파일들이 생성된다
  * log에는 메시지와 메타데이터를 저장
  * index는 메시지의 오프셋을 인덱싱한 정보를 담는다
  * timeindex에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담긴다
  
## 로그와 세그먼트
* 여기서 말하는 로그는 .log 라고하는 세그먼트 파일이다
* 로그는 저장 할 최대크기를 설정 할 수도 있고, 신규 생성 될 다음 파일로 넘어가는 시간 주기를 설정 할 수도있다
  * log.segment.bytes: 바이트 단위의 최대 세그먼트 크기 지정 (기본값은 1GB)
  * log.roll.ms(hours): 세그먼트가 신규 생성된 이후 다음 파일로 넘어가는 시간 주기 (기본값은 7일)
* 가장 마지막 세그먼트 파일(쓰기가 일어나고 있는 파일)을 `액티브 세그먼트`라고 부른다
* 액티브 세그먼트는 브로커의 삭제대상에 포함되지 않는다
* 액티브가 아닌 일반 세그먼트는 retention 옵션에 따라 삭제 대상으로 지정된다
___

## 세그먼트와 삭제 주기(cleanup.policy=delete)
* delete 옵션은 세그먼트 파일을 삭제할때 사용하는 옵션이다(active 제외)
  * retention.ms(minutes, hours): 세그먼트를 보유할 최대 기간(기본값 7일)
  * retention.bytes: 파티션당 로그 적재 바이트 값(기본값 -1, 지정하지 않음을 뜻함)
  * log.retention.check.interval.ms: 세그먼트가 삭제 영역에 들어왔는지 확인하는 간격(기본값 5분)
## 세그먼트의 삭제
### (cleanup.policy=delete)
* 데이터삭제는 세그먼트 단위로만 이뤄지며 로그 단위(레코드 단위)로 개별삭제는 불가능하다
* 또한, 로그(레코드)의 메시지 키, 메시지 값, 오프셋, 헤더 등 이미 적재된 데이터의 수정또한 불가능 하기 때문에<br>프로듀서에서 데이터를 전송하기전, 컨슈머에서 데이터를 가져온 직후 데이터가 정상적인지 검증하는 과정이 필요하다
### (cleanup.policy=compact)
* 토픽 압축 정책은 일반적인 zip압축과는 다른 개념이다
* 하나의 세그먼트에는 동일한 메시지key를 여러 offset에 저장하는 경우가 생기는데 이때 최신 offset을 남겨두고 나머지 offset은 전부 삭제시키는 방식이다
* 물론 이 경우에도 active는 삭제대상에서 제외된다

### 테일/헤드 영역, 클린/더티 로그
* 테일 영역: 압축 정책에 의해 압축이 완료 된 레코드들, 중복 메시지 키가 없기 때문에 클린(clean)로그라고도 부른다
* 헤드 영역: 압축 정책이 되기 전 레코드들, 중복 메시지 키가 존재하기 때문에 더티(dirty)로그 라고도 부른다
* 데이터 압축 시작 시점 : min.cleanable.dirty.ratio 옵션을 따른다
  * 액티브 세그먼트를 제외한 세그먼트에 남아있는 테일 영역의 레코드 개수와 헤드 영역의 레코드 개수의 비율을 뜻한다.
  * 0.5로 설정하면 테일 영역의 레코드 개수와 헤드 영역의 레코드 개수가 동일할 경우 압축이 실행된다
  * 0.9와 같이 크게 설정하면 압축 시 데이터가 많이 줄어들기 때문에 압축효과가 좋으나 0.9비율이 될때까지 용량을 차지한다는 단점이 있다
  * 0.1과 같이 작게 설정하면 압축이 자주 일어나 가장 최신 데이터만 유지할 수 있지만 그만큼 브로커에 부담을 준다
___
## 브로커의 역할 - 복제(Replication)
* 복제를 통해 카프카는 장애허용 시스템(fault tolerant system)으로써 동작할 수 있다
* 클러스터로 묶인 브로커 중 일부에 장애가 발생하더라도 데이터 복제를 통해 데이터 유실을 방지한다
* 복제는 파티션 단위로 이뤄진다
* 토픽을 생성할때 파티션의 복제개수(replication factor)도 같이 설정되는데 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션을 따라간다
* 복제 개수의 최솟값은1(복제없음)이고 최댓값은 브로커 개수만큼 설정 가능하다
* 프로듀서에 의해서 데이터가 처음 저장되는 파티션을 리더 파티션, 복제가 이뤄지는 파티션을 팔로워 파티션 이라고한다, 참고로 컨슈머도 리더 파티션으로 부터 데이터를 가져간다
* replication factor 가 3이고 데이터가 1GB 들어왔다고 가정한다면 나머지 팔로워 파티션에도 각각 1GB가 저장되기 때문에 총 3GB가 저장된다(Disk를 소모하여 고가용성을 챙기는 행위)
* 일반적으로 factor 2를 사용하고 gps 정보와 같이 일부 데이터가 유실되어도 큰 영향이 없는 경우 1, 금융 정보와 같이 유실에 민감한 데이터의 경우 3을 설정한다
* factor 설정값이 커질수록 그만큼 네트워크 통신도 더 발생하고 디스크 소모도 배수로 늘어나는 것을 고려해야한다
___
## ISR(In-Sync-Replicas)
* ISR은 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 뜻한다
* 리더 파티션에 오프셋3까지 데이터가 저장되어있는데 팔로워 파티션에 오프셋2까지만 저장되어 있다면 싱크가 이뤄지지 않았다고 판단한다
* 싱크가 되지 않은 상태에서 팔로워 파티션을 새로운 리더로 선출할 경우가 필요할 수도 있다
(= 서비스를 중단하지 않고 지속적으로 토픽을 사용)
  * unclean.leader.election.enable=true : 유실을 감수함. 복제가 안된 팔로워 파티션을 리더로 승급
  * unclean.leader.election.enable=false : 유실을 감수하지 않음. 해당 브로커가 복구될 때까지 중단
### (ISR이 완전히 이뤄지지 않은 상태에서 컨슈머는 어디까지 데이터를 가져가는가?)
* 토픽에서 min.insync.replicas=2 와 같은 옵션을 설정할 수 있다
* 리터 파티션(오프셋3) / 팔로워 파티션1(오프셋2) / 팔로워 파티션2(오프셋1)의 상태일때 위 옵션에 따라서 오프셋2까지 데이터를 가져가게 되며 옵션값이3 이였다면 오프셋1까지 가져가게된다
* 컨슈머가 가져갈 수 있는 상태의 레코드 오프셋 번호를 하이 워터마크라고 부른다
___
## 토픽과 파티션
* 토픽은 1개이상의 파티션을 소유하고 있다
* 파티션에는 프로듀서가 보낸 데이터가 저장되며 '레코드(record)' 라고 부른다
* 일반적인 자료구조인 큐(FIFO)와 비슷하다 큐와는 다르게 데이터를 꺼낸다고 삭제되진 않는다
* 때문에 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러번 가져갈 수 있다
## 토픽 생성시 파티션이 배치되는 방법
* 파티션이 5개인 토픽을 생성할 경우 0번 브로커부터 2번 브로커까지 round-robin 방식으로 파티션들이 생성된다
* 카프카 클라이언트는 리더 파티션이 있는 브로커와 통신하여 데이터를 주고받기 때문에 여러 브로커에 부하분산을 할 수 있다
* 한마디도 통신이 집중되는(hot spot)현상을 막고 선형 확장(linear scale out)을 하여 데이터가 많아지더라도 자연스럽게 대응할 수 있다
* 특정 브로커에 리더파티션이 몰려 생성되어 있을 경우 해당 브로커의 부하가 커질 수 있기 때문에 kafka-reassign-partitions.sh 명령으로 파티션을 재분배 할 수 있다
### (파티션 개수와 컨슈머 개수의 처리량)
* 한개의 컨슈머는 경우에따라 여러개의 파티션으로 부터 데이터를 가져올 수 있으나 한개의 파티션은 여러개의 컨슈머에게 데이터를 전달 할 수 없다 (1:N 관계)
* 컨슈머 개수를 늘림과 동시에 파티션 개수도 늘리면 처리량 증가의 효과를 볼 수 있다
* 프로듀서가 초당 10개의 데이터를 보내는데 컨슈머가 초당1개의 데이터를 가져갈 경우 데이터 추가에 지연이 발생하고 이를 컨슈머랙이라고 한다
### (파티션 개수를 줄이는 것은 불가능)
* 한번 생성 된 파티션을 줄이는 방법은 토픽을 삭제하고 재생성하는 방법밖에 없기 때문에 파티션을 늘리때는 신중해야한다
___
## 레코드
* 타임스탬프, 헤더, 메시지 키, 메시지 값, 오프셋으로 구성된다
* 프로듀서가 생성한 레코드가 브로커에 전송되면 오프셋과 타임스탬프가 지정되어 저장된다
* 브로커에 한번 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량에 따라서만 삭제된다
### (타임스탬프)
* 스트림 프로세싱에 활용하기 위한 시간을 저장하는 용도로 사용된다
* 카프카 0.10.0.0이후 버전부터 추가된 타임스탬프는 Unix timestamp가 포함된다
* 설정하지 않으면 기본값으로 ProducerRecord 생성시간(CreateTime)이 들어간다
* 또는 브로커 적재 시간(LogAppendTime)으로 설정할 수 있다
* 해당 옵션은 토픽 단위로 설정 가능하며 message.timestamp.type을 사용한다
### (오프셋)
* 오프셋은 프로듀서가 생성한 레코드에는 존재하지 않는다
* 프로듀서가 전송한 레코드가 브로커에 적재될 때 오프셋이 지정된다
* 오프셋은 0부터 시작되고 1씩 증가한다
* 컨슈머는 오프셋을 기반으로 처리 완료된 데이터와 앞으로 처리할 데이터를 구분한다
* 각 메시지는 파티션별로 고유한 오프셋을 가지므로 컨슈머에서 중복 처리를 방지하기 위한 목적으로도 사용한다
### (헤더)
* key/value 데이터를 추가할 수 있다
* 레코드의 스키마 버전이나 포맷과 같이 데이터 프로세싱에 참고할만한 정보를 담아 사용할 수 있다
### (메시지 키)
* 처리하고자 하는 메시지 값을 분류하기 위한 용도로 사용되며, 이를 파티셔닝이라고 부른다.
* 파티셔너(Partitioner)에 따라 토픽의 파티션 번호가 정해진다
* 필수값이 아니며, 지정하지 않으면 null로 설정된다
* 메시지 키가 null인 레코드는 특정 토픽의 파티션에 라운드 로빈으로 전달된다
* null이 아닌 메시지 키는 해쉬값에 의해서 특정 파티션에 매핑되어 전달된다(기본 파티셔너의 경우)
### (메시지 값)
* 메시지 값은 실직적으로 처리할 데이터가 담기는 공간이다
* 메시지 값의 포맷은 제네릭으로 사용자에 의해 지정된다
* Float, Byte[], String 등 다양한 형태로 지정 가능하며 사용자 지정 포맷으로 직렬화/역직렬화 클래스를 만들어 사용할 수도 있다
* 컨슈머는 브로커에 저장된 레코드 메시지 값의 포맷을 알지 못하기 때문에 미리 역직렬화 포맷을 알고 있어야 한다
___
## 토픽이름 제약조건
* 빈 문자열 토픽 이름은 지원하지 않는다
* 토픽 이름은 마침표 하나(.)또는 마침표 둘(..)로 생성될 수 없다
* 토픽 이름의 길이는 249자 미만으로 생성되어야 한다
* 토픽 이름은 영어 대, 소문자와 숫자 0~9 그리고 마침표(.), 언더파(_), 하이픈(-) 조합으로 생성할 수 있다
* 카프카 내부 로직 관리에 사용되는 __consumer_offsets, __transaction_state 와 동일한 이름으로 생성 불가능
* 카프카 내부저긍로 사용하는 로직때문에 마침표(.)와 언더바(_)가 동시에 들어가면 안된다(사용은 가능하나 WARNING메시지 발생)
* 토픽의 이름은 최초 생성이후 변경이 불가능하기 때문에 신중히 작명해야한다
## 토픽 작명의 템플릿과 예시
<환경>.<팀-명>.<애플리케이션-명>.<메시지-타입><br>
예시)prd.marketing-team.sms-platform.json<br>
<프로젝트-명>.<서비스-명>.<환경>.<이벤트-명><br>
예시)commerce.payment.prd.notification<br>
<환경>.<서비스-명>.<JIRA-번호>.<메시지-타입><br>
예시)dev.email-sender.jira-1234.email-vo.custom<br>
<카프카-클러스터-명>.<환경>.<서비스-명>.<메시지-타입><br>
예시)aws-kafka.live.marketing-platform.json<br>
___
## 클라이언트 메타데이터
* 카프카 클라이언트는 통신하고자 하는 리더 파티션의 위치를 알기 위해 데이터를 주고(프로듀서) 받기(컨슈머) 전에 메타데이터를 브로커로부터 전달받는다
* 메타데이터는 다음과 같은 옵션을 통해 리프래쉬된다
  * 카프카 프로듀서 메타데이터 옵션
  * metadata.max.age.ms : 메타데이터를 강제로 리프래시하는 간격(기본값 5분)
  * metadata.max.idle.ms : 프로듀서가 유휴상태일 경우 메타데이터를 캐시에 유지하는 기간. 예를들어 프로듀서가 특정 토픽으로 데이터를 보낸 이후 지정한 시간이 지나고 나면 강제로 메타데이터를 리프래쉬(기본값 5분)
## 클라이언트 메타데이터의 이슈가 발생한 경우
* 카프카 클라이언트는 반드시 리더파티션과 통신해야 한다
* 메타데이터가 최신상태로 리프래쉬되지 않아 잘못된 브로커에 요청할 경우 LEADER_NOT_AVAILABLE 익셉션이 발생한다
* 보통 클라이언트가 데이터를 요청한 브로커에 리더파티션이 없는 경우 나타나며 대부분 메타데이터 리프래쉬 이슈로 발생한다
* 이 에러가 자주 발생한다면 메타데이터 리프래쉬 간격을 확인하고, 클라이언트가 정상적인 메타데이터를 가지고 있는지 확인한다
___
## 카프카 클러스터를 운영하는 방법
```
아파치 카프카 클러스터를 서버에 직접 설치하고 운영하는것은 일반적인 방법이지만
최적화된 카프카 클러스터를 사용하기 위해서는 노하우가 필요하기 때문에 SaaS(Software-as-a-Service)를 도입하는 것을 고려해보자.
```
 ### (오픈소스 카프카 VS 기업용 카프카(컨플루언트 플랫폼))
 오픈소스 카프카는 기본적인 프로듀서, 컨슈머, 커넥트, 스트림즈를 제공하며 기업용도 마찬가지이다 기업용 카프카의 차이점이라면 환경에 맞추어 제공되는 플러그인, 모니터링툴, 최적의 설정 노하우 등의 서비스를 유료로 제공받을 수 있다.

 ### 오픈 소스 카프카를 직접 설치하여 운영하는 경우
|항목|개발용 카프카 클러스|상용 환경 카프카 클러스터|
|-|-|-|
|브로커 개수|5개|10개|
|메모리|16GB(heap memory 6GB)|32GB(heap memory 6GB)|
|CPU|16core|24core|
|디스크|사용량에 따라 달라짐|사용량에 따라 달라짐|
___
## 클라우드 서비스 - 컨플루언트
```
카프카에 대한 개념을 최초로 생각하고 아키텍처를 제안, 개발한 인물인 제이 크랩스와 그의 동료들이 설립한 회사이다. 아파치 카프카의 생태계를 가꾸고 발전시키는 데에 선구적인 역할을 하고 있다. 컨플루언트에서 오픈소스로 공개하고 관리하고 있는 스키마 레지스트리, ksqlDB, connector, restproxy 등과 같은 소스 코드들은 카프카를 활용하는 생태계의 범위를 점차 늘려가고 있다.
```
|<center>컨플루언트 클라우드</center>|<center>컨플루언트 플랫폼</center>|
|-|-|
|- 클라우드 기반 카프카 클러스터|- 온프레미스 기반 설치형 카프카 클러스터|
|- 요구사항에 따라 자동으로 늘려주는 클러스터 리소스 제공|- 서버를 내부에서 발급하여 직접 설치|
|- GCP, AWS등 클러스터 설치 위치 지정(리전 단위) 가능|- 필요에 따라 컨플루언트 팀에서 지원, 학습 제공|
|- 120개가 넘는 커넥터, ksqlDB, 스키마 레지스트리 서비스 제공|- 단계별 스토리지 기능(Tiered-storage)제공|
|- 99.95% SLA |- GUI 기반 모니터링 시스템 제공|
|- 엔터프라이즈 수준의 보안 수준 제공|
|- 데이터 적재 제한 없음||
___
## 클라우드 서비스 - AWS MSK
* Amazon Managed Streaming for Apache Kafka
* 카프카 클러스터의 생성, 업데이트, 삭제 등과 같은 운영 요소를 대시보드를 통해 제공한다
* 안전하게 접속할 수 있도록 클러스터와 연동시 TLS인증 보안을 설정 할 수 있다
* 아파치 카프카 버전을 직접 선택할 수 있다
* 따라서 이미 운영중이던 카프카를 연동할때 버전이슈를 발생시키지 않는다
___
## SaaS로 카프카 클러스터를 운영할 경우 장점
(인프라 관리의 효율화)

* 카프카 클러스터는 상용환경에서 최소 broker 서버 3대 2버전 이하라면 zookeeper 3대를 함께 운영 관리해야하는데, 이렇게 많은 서버를 운영자가 관리하는것은 굉장히 힘든 일이다.
* SaaS 환경에서는 서비스 이슈에 대해서 자동으로 감지하고 Scale in/out 이 자유롭기 때문에 운영관리함에 있어 훨씬 효율적이다


(모니터링 대시보드 제공)
* 브로커들이 제공하는 지표들을 수집하고 적재하고 대시보드화 하여 시각화해야 카프카 클러스터를 효과적으로 운영하기 위한 설정들을 수정하고 적용할 수 있을것이다. 
* 운형 노하우가 없다면 어떤 지표들을 수집해야하는지, 어떤 지표가 중요한지 알아가는데에도 시간이 걸리며, 수집한 지표를 저장할 저장소를 구축하고 대시보드를 운영하기 위해 신규로 추가 플랫폼을 설치, 운영해야한다. 
* SaaS형 카프카에서는 이러한 부분들을 단지 몇번의 클릭으로 해결할 수 있다.

(보안설정)
* 보안이 설정되지 않은 클러스터의 경우 호스트와 포트 번호만 알면 모든 토픽의 데이터를 가져갈 수 있다.
* 또한, 어드민API를 통해 카프카 클러스터에 악의적인 공격을 할 수도 있다. 
* 카프카 브로커는 SSL, SASL, ACL과 같이 불특정 다수의 침입을 막기위해 다양한 종류의 보안 설정 방안을 제공한다. 
* 이또한 노하우가 없다면 어떤 종류의 보안설정을 할지 고르고 설정, 운영하는 것은 쉬운일이 아니다. 
* SaaS형에서는 클러스터 접속 시 보안 설정을 기본으로 제공하고 있다. 
* 클러스터 생성 시 보안 설정을 통해 인가된 사용자만 카프카 클러스터에 접근할 수 있도록 할 수 있다.
___
## SaaS로 카프카 클러스터를 운영할 경우 단점
(서비스 사용 비용)
* 직접 설치하고 운영한다면 발생되지 않을 서버 사용 비용이 추가로 발생한다
* 인스턴스 발급 및 사용 비용은 클러스터를 단순히 실행만 하더라도 빠져나가는 비용이며 추가적으로 스토리지 요금과 데이터 전송 요금이 발생한다

(커스터마이징의 제한)
* 카프카 클러스터를 운영하다 보면 서버의 최적화 옵션이나 카프카 브로커 옵션의 조정이 필요한 경우가 발생한다
* SaaS 서비스들은 자동화된 설정에 맞춰 아키텍처를 따라가기 때문에 상세한 설정을 적용하거나 클러스터 아키텍처의 변화가 필요할 경우 적용하기가 매우 어렵다
* 멀티 클라우드, 하이브리드 클라우드 형태로 카프카 클러스터를 구성하는 것은 SaaS형에서 불가능하다.